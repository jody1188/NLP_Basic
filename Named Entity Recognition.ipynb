{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ec01c1",
   "metadata": {},
   "source": [
    "## 개체명 인식 (Named Entity Recognition)\n",
    "\n",
    "#### 철수 -이름 / 밥 - 사물"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8917fbb",
   "metadata": {},
   "source": [
    "### 1. NlTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5d4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/jody/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jody/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jody/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jody/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8a089",
   "metadata": {},
   "source": [
    "### 토큰화 및 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad57622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d15c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jame', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Diseny', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Jame is working at Diseny in London'\n",
    "sentence = pos_tag(word_tokenize(sentence))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979ff0c",
   "metadata": {},
   "source": [
    "### 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515b795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Jame/NNP)\n",
      "  is/VBZ\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Diseny/NNP)\n",
      "  in/IN\n",
      "  (GPE London/NNP))\n"
     ]
    }
   ],
   "source": [
    "sentence = ne_chunk(sentence)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee11f6f",
   "metadata": {},
   "source": [
    "### 개체명 인식 (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3f1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3373199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = []\n",
    "sentence = []\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8')\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.strip().split(' ')\n",
    "        word = splits[0].lower()\n",
    "        sentence.append([word, splits[-1]]) # 단어랑 개체명\n",
    "        \n",
    "print(len(tagged_sentences))\n",
    "print(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6dc2cd",
   "metadata": {},
   "source": [
    "#### Data preprocessing \n",
    "##### - 단어와 개체명 태그를 분리해서 데이터를 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c3ced24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, ner_tags = [], []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tag_info = zip(*tagged_sentence) # 단어들은 sentence / 개체명은 tag_info에\n",
    "    sentences.append(list(sentence))\n",
    "    ner_tags.append(list(tag_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995e91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4000\n",
    "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')\n",
    "src_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73160e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_size)\n",
    "print(tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a89d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tar_tokenizer.texts_to_sequences(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b48d707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "X_train = pad_sequences(X_train, padding = 'post', maxlen = max_len)\n",
    "y_train = pad_sequences(y_train, padding = 'post', maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7e11d",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f2098fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 111)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc2bdf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11232, 70)\n",
      "(11232, 70, 10)\n",
      "(2809, 70)\n",
      "(2809, 70, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "630fc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fbb0254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 13:05:18.614519: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length = max_len, mask_zero = True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d38a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 70, 128)           512000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 70, 512)          788480    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 70, 10)           5130      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,305,610\n",
      "Trainable params: 1,305,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb5547b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "88/88 [==============================] - 45s 466ms/step - loss: 0.1848 - accuracy: 0.8254 - val_loss: 0.1321 - val_accuracy: 0.8311\n",
      "Epoch 2/3\n",
      "88/88 [==============================] - 42s 476ms/step - loss: 0.0976 - accuracy: 0.8562 - val_loss: 0.0755 - val_accuracy: 0.8878\n",
      "Epoch 3/3\n",
      "88/88 [==============================] - 46s 525ms/step - loss: 0.0638 - accuracy: 0.9062 - val_loss: 0.0551 - val_accuracy: 0.9198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe03b1c5760>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer = Adam(0.001),\n",
    "             metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07d34ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 9s 96ms/step - loss: 0.0551 - accuracy: 0.9198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05507287010550499, 0.9198400974273682]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170a343c",
   "metadata": {},
   "source": [
    "### 학습한 모델을 통한 예측\n",
    "#### - 예측을 확인하기 위해서 인덱스를 단어로 변환해줄 사전이 필요\n",
    "#### - 사전은 토큰화 툴의 사전을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd3b6956",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = src_tokenizer.index_word\n",
    "idx2ner = tar_tokenizer.index_word\n",
    "idx2ner[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b29db",
   "metadata": {},
   "source": [
    "### 예측 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68d36ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어             |실제값  |예측값\n",
      "------------------------------\n",
      "dhaka            : B-LOC   B-LOC\n",
      "1996-08-22       : O       O\n"
     ]
    }
   ],
   "source": [
    "i = 50\n",
    "y_predicted = model.predict(np.array([X_test[i]]))\n",
    "y_predicted = np.argmax(y_predicted, axis=-1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "\n",
    "print(\"{:15}|{:5}|{}\".format('단어', '실제값', '예측값'))\n",
    "print('-' * 30)\n",
    "\n",
    "for w, t, pred in zip(X_test[i], true, y_predicted[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:17}: {:7} {}\".format(idx2word[w], idx2ner[t].upper(), idx2ner[pred].upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad48a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
