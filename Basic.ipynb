{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1eb6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited UK from US on 22-09-20\n"
     ]
    }
   ],
   "source": [
    "s = \"I visited UK from US on 22-09-20\"\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fe460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited Uinted Kingdom from United States on 22-09-2020\n"
     ]
    }
   ],
   "source": [
    "new_s = s.replace('UK', 'Uinted Kingdom').replace('US', 'United States').replace('-20','-2020')\n",
    "print(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a220f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657c157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "check = 'ab.'\n",
    "\n",
    "print(re.match(check, 'abc'))\n",
    "print(re.match(check, 'c'))\n",
    "print(re.match(check, 'ab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e535568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cddc66cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일반 사용시 소요 시간 :  0.0010838508605957031\n",
      "컴파일 사용시 소요 시간 0.0005297660827636719\n"
     ]
    }
   ],
   "source": [
    "normal_s_time = time.time()\n",
    "r = 'ab'\n",
    "\n",
    "for i in range(1000):\n",
    "    re.match(check, 'abc')\n",
    "print('일반 사용시 소요 시간 : ', time.time()-normal_s_time)\n",
    "\n",
    "compile_s_time = time.time()\n",
    "r = re.compile('ab.')\n",
    "for i in range(1000):\n",
    "    r.match(check)\n",
    "print('컴파일 사용시 소요 시간', time.time() - compile_s_time)\n",
    "\n",
    "# 반복 사용시 compile을 사용하는게 좋다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "172e0aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ab'>\n"
     ]
    }
   ],
   "source": [
    "check = 'ab?'\n",
    "\n",
    "print(re.search('a', check))\n",
    "print(re.match('kkkab', check))\n",
    "print(re.match('kkkab', check))\n",
    "print(re.match('ab', check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb0e1980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abbc', 'abcab']\n",
      "['ab', ' abb', ' ab', 'bab']\n",
      "['s', 'abc ', 'v', 's ', 'sss ', 'a']\n"
     ]
    }
   ],
   "source": [
    "r = re.compile(' ')\n",
    "print(r.split('abc abbc abcab'))\n",
    "\n",
    "r = re.compile('c')\n",
    "print(r.split('abc abbc abcbab'))\n",
    "\n",
    "r = re.compile('[1-9]')\n",
    "print(r.split('s1abc 2v3s 4sss 5a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72dfdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abc defg\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[a-z]', 'abcdefg', '1'))\n",
    "\n",
    "print(re.sub('[^a-z]', 'abc defg', '1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c3d61f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4']\n",
      "['!', '@', '@', '#']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('[\\d]', '1ab 2cd 3ef 4g')) # 숫자만\n",
    "\n",
    "print(re.findall('[\\W]', '!abcd@@#')) # 특수문자만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e636e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x7ff6488e3d60>\n",
      "<re.Match object; span=(0, 1), match='1'>\n",
      "<re.Match object; span=(4, 5), match='2'>\n",
      "<re.Match object; span=(8, 9), match='3'>\n",
      "<re.Match object; span=(12, 13), match='4'>\n",
      "<callable_iterator object at 0x7ff5e956e100>\n",
      "<re.Match object; span=(0, 1), match='!'>\n",
      "<re.Match object; span=(5, 6), match='@'>\n",
      "<re.Match object; span=(6, 7), match='@'>\n",
      "<re.Match object; span=(7, 8), match='#'>\n"
     ]
    }
   ],
   "source": [
    "iter1 = re.finditer('[\\d]', '1ab 2cd 3ef 4g')\n",
    "print(iter1)\n",
    "for i in iter1:\n",
    "    print(i)\n",
    "    \n",
    "iter2 = re.finditer('[\\W]', '!abcd@@#')\n",
    "print(iter2)\n",
    "for i in iter2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063a938",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80194556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Time is gold'\n",
    "tokens = [x for x in sentence.split(' ')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4748f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jody/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "003c9c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "716f8143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a beautiful book.\n",
      "But of little use to him who cannot read it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful book.',\n",
       " 'But of little use to him who cannot read it']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = 'The world is a beautiful book.\\nBut of little use to him who cannot read it'\n",
    "print(sentences)\n",
    "\n",
    "tokens = [x for x in sentences.split('\\n')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2233f51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful book.',\n",
       " 'But of little use to him who cannot read it']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokens = sent_tokenize(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ba7b0",
   "metadata": {},
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\") # 문자와 숫자만\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b565b663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\s]+\", gaps= True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23f7750c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"Where there\\'s a will, there\\'s a way\"\n",
    "\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a61a8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28214264",
   "metadata": {},
   "source": [
    "## ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8db9dcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'There is no royal road to learning'\n",
    "bigram = list(ngrams(sentence.split(), 2)) # 두개씩 묶어서 만든다\n",
    "print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eb05ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'learning')]\n"
     ]
    }
   ],
   "source": [
    "trigram = list(ngrams(sentence.split(),3))\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb3f02bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['There', 'is']),\n",
       " WordList(['is', 'no']),\n",
       " WordList(['no', 'royal']),\n",
       " WordList(['royal', 'road']),\n",
       " WordList(['road', 'to']),\n",
       " WordList(['to', 'learning'])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87fa5a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['There', 'is', 'no']),\n",
       " WordList(['is', 'no', 'royal']),\n",
       " WordList(['no', 'royal', 'road']),\n",
       " WordList(['royal', 'road', 'to']),\n",
       " WordList(['road', 'to', 'learning'])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05c46d",
   "metadata": {},
   "source": [
    "## PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfd29db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jody/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dded6f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'action',\n",
       " 'and',\n",
       " 'act',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'thought',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = word_tokenize('Think like man of action and act like man of thought.')\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cff3abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jody/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Think', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('action', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('thought', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.pos_tag(word) # 품사가 붙음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25cac467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('rolling', 'VBG'),\n",
       " ('stone', 'NN'),\n",
       " ('gathers', 'NNS'),\n",
       " ('no', 'DT'),\n",
       " ('moss', 'NN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize('A rolling stone gathers no moss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444b147",
   "metadata": {},
   "source": [
    "## 불용어 제거 (조사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb83eda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on', 'in', 'the']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = 'on in the'\n",
    "stop_words = stop_words.split(' ')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4159ee34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singer', 'stage']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'singer on the stage'\n",
    "sentence = sentence.split(' ')\n",
    "nouns = []\n",
    "for noun in sentence:\n",
    "    if noun not in stop_words:\n",
    "        nouns.append(noun)\n",
    "        \n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "219a9757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jody/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9962378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe7c5624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'do', 'not', 'walk', 'today', ',', 'you', 'will', 'have', 'to', 'run', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "s = 'If you do not walk today, you will have to run tomorrow.'\n",
    "words = word_tokenize(s)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ff3f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'walk', 'today', ',', 'run', 'tomorrow', '.']\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        no_stopwords.append(w)\n",
    "        \n",
    "print(no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6389551",
   "metadata": {},
   "source": [
    "## 철자 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6f6d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "\u001b[K     |████████████████████████████████| 622 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622381 sha256=0115e14991c7c6a22d08bd0f1d2692fa54b2e21ff8754fe4ecad029bec745fca\n",
      "  Stored in directory: /Users/jody/Library/Caches/pip/wheels/72/b8/3b/a90246d13090e85394a8a44b78c8abf577c0766f29d6543c75\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3066996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "913b5aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n",
      "people\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "spell = Speller('en')\n",
    "\n",
    "print(spell('peoplle'))\n",
    "print(spell('peope'))\n",
    "print(spell('peopae'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "412c01c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Earlly', 'biird', 'catchess', 'the', 'womm']\n",
      "Early bird catches the worm\n"
     ]
    }
   ],
   "source": [
    "s = word_tokenize('Earlly biird catchess the womm')\n",
    "print(s)\n",
    "ss = ' '.join([spell(s) for s in s])\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e80c97",
   "metadata": {},
   "source": [
    "## 언어의 단수화와 복수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0123e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'bannas', 'oranges']\n",
      "['apple', 'banna', 'orange']\n"
     ]
    }
   ],
   "source": [
    "words = 'apples bannas oranges'\n",
    "tb = TextBlob(words)\n",
    "\n",
    "print(tb.words)\n",
    "print(tb.words.singularize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b582a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'train', 'airplane']\n",
      "['cars', 'trains', 'airplanes']\n"
     ]
    }
   ],
   "source": [
    "words = 'car train airplane'\n",
    "tb = TextBlob(words)\n",
    "\n",
    "print(tb.words)\n",
    "print(tb.words.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95096fa0",
   "metadata": {},
   "source": [
    "## 어간(Stemming) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4cb8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1cbcfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'applic'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e1661da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'begin'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('beginning') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f393514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('catches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9728b43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'educ'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('education')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2f2ae",
   "metadata": {},
   "source": [
    "## 표제어(Lemmatization) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c764031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jody/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a79decf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('application')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b2e2c5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beginning'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('beginning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d098c64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catch'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('catches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "875ec687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'education'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('education')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f4dc0",
   "metadata": {},
   "source": [
    "## 개체명 인식(Named Entity Recognization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4be5c4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/jody/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/jody/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5bf4257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome was not bulit in a day\n"
     ]
    }
   ],
   "source": [
    "s = 'Rome was not bulit in a day'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5fa61773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('bulit', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(word_tokenize(s))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a06f773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NE Rome/NNP) was/VBD not/RB bulit/VBN in/IN a/DT day/NN)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.ne_chunk(tags, binary=True)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64feb8a",
   "metadata": {},
   "source": [
    "## 단어의 중의성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aac8930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'saw', 'bats']\n",
      "Synset('saw.v.01')\n",
      "Synset('squash_racket.n.01')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "s = 'I saw bats'\n",
    "\n",
    "print(word_tokenize(s))\n",
    "print(lesk(word_tokenize(s), 'saw'))\n",
    "print(lesk(word_tokenize(s), 'bat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d680b16",
   "metadata": {},
   "source": [
    "# 한국어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70de4409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='ㅎ'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "check = '[ㄱ-ㅎ]+'\n",
    "\n",
    "print(re.match(check, 'ㅎ 안녕하세요.'))\n",
    "print(re.match(check, '안녕하세요 ㅎ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cd6e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ㄱㅏ'>\n",
      "None\n",
      "<re.Match object; span=(2, 4), match='ㄱㅏ'>\n"
     ]
    }
   ],
   "source": [
    "check = '[ㄱ-ㅎ:ㅏ-ㅣ]+'\n",
    "\n",
    "print(re.search(check, 'ㄱㅏ 안녕하세요'))\n",
    "print(re.match(check, '안 ㄱㅏ '))\n",
    "print(re.search(check, '안 ㄱㅏ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56124d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "가나다라마바사\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[가-힣]', '가나다라마바사', '1'))\n",
    "print(re.sub('[^가-힣]', '가나다라마바사', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c89859",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6369182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.3.1.1.tar.gz (42.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.4 MB 54.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting emoji\n",
      "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
      "\u001b[K     |████████████████████████████████| 170 kB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: kss, emoji\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kss: filename=kss-3.3.1.1-py3-none-any.whl size=42449239 sha256=14de754347162bfc0a30f1728ecb40a1321aa9bcc16ff544e5044a63b26f9743\n",
      "  Stored in directory: /Users/jody/Library/Caches/pip/wheels/02/b1/a4/6c41b81f6f42a8301b50c37e70842e20d00510fd0cf7f816ea\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=46524c52b7f05097af2596886dc70740b80a4eb634980fc65a60cc726196611d\n",
      "  Stored in directory: /Users/jody/Library/Caches/pip/wheels/04/29/50/1e7189f03d2cf139e469863d54a1d3eabeb10c92c84e51f8a1\n",
      "Successfully built kss emoji\n",
      "Installing collected packages: emoji, kss\n",
      "Successfully installed emoji-1.6.1 kss-3.3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c1e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['진짜? 내일 뭐하지.', '이렇게 애매모호한 문장도? 밥은 먹었어?', '나는...']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '진짜? 내일 뭐하지. 이렇게 애매모호한 문장도? 밥은 먹었어? 나는...'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3c2cc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요', '저는', '자연어', '처리', '를', '배우고', '있습니다']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = '안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다'\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[가-힣]+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd277b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요 ', ' 저는 자연어 처리(Natural Language Processing)를', '!! 배우고 있습니다']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('[ㄱ-ㅎ]+', gaps = True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0a621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['성공의', '비결을', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = '성공의 비결을 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17a2a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['성공의', '비결을', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61319c37",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3506904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 2 2 1 1]]\n",
      "{'think': 6, 'like': 3, 'man': 4, 'of': 5, 'action': 1, 'and': 2, 'act': 0, 'thought': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['Think like a man of action and act like man of thought.']\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a2110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 2 2 1 1]]\n",
      "{'think': 6, 'like': 3, 'man': 4, 'of': 5, 'action': 1, 'and': 2, 'act': 0, 'thought': 7}\n"
     ]
    }
   ],
   "source": [
    "vetor = CountVectorizer(stop_words='english')\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_) # of가 빠짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "485d21c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 1 1 1 1]]\n",
      "{'평생': 8, '것처럼': 0, '꿈을': 3, '꾸어라': 2, '그리고': 1, '내일': 4, '죽을': 7, '오늘을': 6, '살아라': 5}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라']\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30e6828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'평생': 0, '살': 1, '것': 2, '처럼': 3, '꿈': 4, '을': 5, '꾸': 6, '어라': 7, '그리고': 8, '내일': 9, '죽': 10, '오늘': 11, '아라': 12}\n",
      "['평생', '살', '것', '처럼', '꿈', '을', '꾸', '어라', '그리고', '내일', '죽', '을', '것', '처럼', '오늘', '을', '살', '아라']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "tagger = Mecab()\n",
    "\n",
    "corpus = '평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라'\n",
    "tokens = tagger.morphs(re.sub(\"\\.\", \"\", corpus))\n",
    "\n",
    "vocab = {}\n",
    "bow = []\n",
    "\n",
    "for tok in tokens:\n",
    "    if tok not in vocab.keys():\n",
    "        vocab[tok] = len(vocab)\n",
    "        bow.insert(len(vocab)-1,1)\n",
    "    else:\n",
    "        index = vocab.get(tok)\n",
    "        bow[index] = bow[index]+1\n",
    "print(bow)\n",
    "print(vocab)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75276a",
   "metadata": {},
   "source": [
    "## 문서 단어 행렬 (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f0c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 2 2 0 1 1 0 0]\n",
      " [0 0 0 0 0 2 1 0 0 2 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 0]]\n",
      "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['Think like a man of action and act like man of thought.',\n",
    "         'Try not to become a man of success but rather try to become a man of value.',\n",
    "         'Give me liberty, of give me death']\n",
    "\n",
    "vector = CountVectorizer(stop_words = 'english')\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9616b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>death</th>\n",
       "      <th>liberty</th>\n",
       "      <th>like</th>\n",
       "      <th>man</th>\n",
       "      <th>success</th>\n",
       "      <th>think</th>\n",
       "      <th>thought</th>\n",
       "      <th>try</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   act  action  death  liberty  like  man  success  think  thought  try  value\n",
       "0    1       1      0        0     2    2        0      1        1    0      0\n",
       "1    0       0      0        0     0    2        1      0        0    2      1\n",
       "2    0       0      1        1     0    0        0      0        0    0      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = []\n",
    "for k,v in sorted(vector.vocabulary_.items(), key = lambda item : item[1]):\n",
    "    columns.append(k)\n",
    "\n",
    "df = pd.DataFrame(bow.toarray(), columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa29dd3",
   "metadata": {},
   "source": [
    "## 어휘 빈도-문서 역빈도(TF-IDF)분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a34a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.311383   0.311383   0.         0.         0.62276601 0.4736296\n",
      "  0.         0.311383   0.311383   0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.52753275\n",
      "  0.34682109 0.         0.         0.69364217 0.34682109]\n",
      " [0.         0.         0.70710678 0.70710678 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "{'think': 7, 'like': 4, 'man': 5, 'action': 1, 'act': 0, 'thought': 8, 'try': 9, 'success': 6, 'value': 10, 'liberty': 3, 'death': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english').fit(corpus)\n",
    "\n",
    "print(tfidf.transform(corpus).toarray())\n",
    "print(tfidf.vocabulary_) #tf-idf 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c77f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>death</th>\n",
       "      <th>liberty</th>\n",
       "      <th>like</th>\n",
       "      <th>man</th>\n",
       "      <th>success</th>\n",
       "      <th>think</th>\n",
       "      <th>thought</th>\n",
       "      <th>try</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.473630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693642</td>\n",
       "      <td>0.346821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        act    action     death   liberty      like       man   success  \\\n",
       "0  0.311383  0.311383  0.000000  0.000000  0.622766  0.473630  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.527533  0.346821   \n",
       "2  0.000000  0.000000  0.707107  0.707107  0.000000  0.000000  0.000000   \n",
       "\n",
       "      think   thought       try     value  \n",
       "0  0.311383  0.311383  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.693642  0.346821  \n",
       "2  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = []\n",
    "for k,v in sorted(tfidf.vocabulary_.items(), key = lambda item : item[1]):\n",
    "    columns.append(k)\n",
    "\n",
    "df = pd.DataFrame(tfidf.transform(corpus).toarray(), columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc88ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
